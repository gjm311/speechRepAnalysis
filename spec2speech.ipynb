{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gabriel\\Anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from scipy.io.wavfile import read\n",
    "import scipy\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "from librosa.feature import melspectrogram\n",
    "import scaleogram as scg \n",
    "from logmmse import logmmse_from_file\n",
    "from scipy.signal import butter, lfilter\n",
    "import pywt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sys\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gabriel\\Anaconda3\\lib\\site-packages\\torchaudio\\extension\\extension.py:14: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n"
     ]
    }
   ],
   "source": [
    "from AEspeech import AEspeech\n",
    "from SpecDatset import SpecDataset\n",
    "from CAE import CAEn\n",
    "from diffwave.inference import predict as diffwave_predict\n",
    "\n",
    "PATH=os.getcwd()\n",
    "sys.path.append(PATH+\"/toolbox/\")\n",
    "import traintestsplit as tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-15273bc6ba25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPATH\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/diff_models/weights-87462.pt\"\u001b[0m \u001b[1;31m#'/path/to/model/dir'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0maudio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiffwave_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspectrogram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\diffwave\\inference.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(spectrogram, model_dir, params, device)\u001b[0m\n\u001b[0;32m     58\u001b[0m       \u001b[0mc1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m       \u001b[0mc2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0malpha_cum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m       \u001b[0maudio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0maudio\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mc2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspectrogram\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maudio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mnoise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\diffwave\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, audio, spectrogram, diffusion_step)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[0mskip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresidual_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m       \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_connection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspectrogram\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiffusion_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m       \u001b[0mskip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mskip_connection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\diffwave\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, conditioner, diffusion_step)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_projection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[0mresidual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mresidual\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path_audio=PATH+'/tedx_spanish_corpus/speech/train/'\n",
    "wav_file=path_audio+os.listdir(path_audio)[3]\n",
    "\n",
    "nb=1\n",
    "FS=16000\n",
    "fs_in, signal=read(wav_file)\n",
    "sig_len=len(signal)\n",
    "\n",
    "#binary narrowband: 1 yes, 0 no (i.e. broadband)\n",
    "if nb==0:\n",
    "    #broadband: higher time resolution, less frequency resolution\n",
    "    NMELS=64\n",
    "    HOP=int(FS*.003)#3ms hop (48 SAMPLES)\n",
    "    WIN_LEN=int(FS*.005)#5ms time window (60 SAMPLES)\n",
    "    signal=butter_bandpass_filter(signal,50,7000,FS)\n",
    "elif nb==1:\n",
    "    #narrowband: higher frequency resolution, less time resolution\n",
    "    NMELS=128\n",
    "    HOP=int(FS*.01) #10ms hop (160 SAMPLES)\n",
    "    WIN_LEN=int(FS*.03) #30ms time window (480 SAMPLES)\n",
    "    signal=butter_bandpass_filter(signal,300,5400,FS)\n",
    "\n",
    "if fs_in!=FS:\n",
    "    raise ValueError(str(fs)+\" is not a valid sampling frequency\")\n",
    "\n",
    "signal=signal-np.mean(signal)\n",
    "signal=signal/np.max(np.abs(signal))\n",
    "# init=0\n",
    "# endi=int(FRAME_SIZE*sig_len)\n",
    "# nf=int(1/TIME_SHIFT)-1\n",
    "\n",
    "\n",
    "imag=melspectrogram(signal, sr=FS, n_fft=1024,win_length=WIN_LEN, hop_length=HOP, n_mels=NMELS, fmax=FS//2)\n",
    "\n",
    "imag=np.abs(imag)\n",
    "imag=np.log(imag, dtype=np.float32)\n",
    "spectrogram=torch.from_numpy(imag)\n",
    "\n",
    "model_dir=PATH+\"/diff_models/weights-87462.pt\" #'/path/to/model/dir'\n",
    "audio, sample_rate = diffwave_predict(spectrogram.float(), model_dir, device=torch.device('cpu'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicubic_img.shape\n",
    "# spectrogram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=os.getcwd()\n",
    "path_audio=PATH+'/tedx_spanish_corpus/speech/test/'\n",
    "wav_file=path_audio+os.listdir(path_audio)[0]\n",
    "\n",
    "fs_in, signal=read(wav_file)\n",
    "\n",
    "phon=Phonet()\n",
    "\n",
    "#set loop parameters\n",
    "rep='spec'\n",
    "models=['CAE','RAE']\n",
    "mod='CAE'\n",
    "#     units=[]\n",
    "unit=256\n",
    "num_files=len(os.listdir(path_audio))\n",
    "\n",
    "save_path=PATH+'/phonCSVs/'+rep+'/'\n",
    "\n",
    "FS=16000\n",
    "NFFT=512  \n",
    "HOP=64\n",
    "\n",
    "# for mod in models:\n",
    "aespeech=AEspeech(model=mod,units=unit,rep=rep)\n",
    "if rep=='spec':\n",
    "    mat=aespeech.compute_spectrograms(wav_file)\n",
    "    #mat=aespeech.standard(mat)\n",
    "if rep=='wvlt':\n",
    "    mat=aespeech.compute_cwt(wav_file)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    mat=mat.cuda()\n",
    "to,bot=aespeech.AE.forward(mat)\n",
    "# to=aespeech.destandard(to)\n",
    "# mat=aespeech.destandard(mat)\n",
    "\n",
    "spec_recon=to.cpu().data.numpy()\n",
    "spec_ori=mat.cpu().data.numpy()\n",
    "\n",
    "nf=spec_recon.shape[0]\n",
    "\n",
    "fs_in, audio=read(wav_file)\n",
    "\n",
    "\n",
    "\n",
    "# speech_recon=mel2speech(spec_recon)\n",
    "# speech_ori=mel2speech(spec_ori)\n",
    "    \n",
    "# if not os.path.isdir(save_path):\n",
    "#     os.makedirs(save_path)\n",
    "\n",
    "# reconPath_phonSave=save_path+'/'+str(unit)+'_'+mod+'_recon.csv'\n",
    "# oriPath_phonSave=save_path+'/'+str(unit)+'_'+mod+'_original.csv'    \n",
    "# phon.get_phon_wav(speech_recon,feat_file=reconPath_phonSave,phonclass=\"all\")\n",
    "# phon.get_phon_wav(speech_ori,feat_file=oriPath_phonSave,phonclass=\"all\")\n",
    "\n",
    "#     print(\"processing file \", j+1, \" from \", str(num_files), \" \", hf[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=PATH+\"/waveglow/mel_tests/\"\n",
    "files=file_path+os.listdir(file_path)[0]\n",
    "\n",
    "aespeech.show_spectrograms(mat,to)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmax=2595*np.log10(1+8000/700)\n",
    "m=np.linspace(0,mmax,11)\n",
    "\n",
    "f=np.round(700*(10**(m/2595)-1))\n",
    "f=f[::-1]\n",
    "for k in range(spec_ori.shape[0]):\n",
    "    fig,ax=plt.subplots(1,2)\n",
    "    fig.set_size_inches(5, 5)\n",
    "    mat=spec_ori[k,0,:,:]\n",
    "#     mat=spectrograms.data.numpy()[k,0,:,:]\n",
    "    ax_curr=ax[]\n",
    "    plt.imshow(np.flipud(mat), cmap=plt.cm.viridis, vmax=mat.max())\n",
    "    ax.set_yticks(np.linspace(0,128,11))\n",
    "    ax.set_yticklabels(map(str, f))\n",
    "    ax.set_xticks(np.linspace(0,126,6))\n",
    "    ax.set_xticklabels(map(str, np.linspace(0,500,6, dtype=np.int)))\n",
    "    ax.set_ylabel(\"Frequency (Hz)\")\n",
    "    ax.set_xlabel(\"Time (ms)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dct(n_mfcc,n_mel):\n",
    "    basis = np.empty((n_mfcc, n_mel))\n",
    "    basis[0, :] = 1.0 / np.sqrt(n_mel)\n",
    "\n",
    "    samples = np.arange(1, 2*n_mel, 2) * np.pi / (2.0 * n_mel)\n",
    "\n",
    "    for i in range(1, n_mfcc):\n",
    "        basis[i, :] = np.cos(i*samples) * np.sqrt(2.0/n_mel)\n",
    "\n",
    "    return basis\n",
    "\n",
    "def invlogamplitude(S):\n",
    "    \"\"\"librosa.logamplitude is actually 10_log10, so invert that.\"\"\"\n",
    "    return 10.0**(S/10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel2speech(spec_sig):\n",
    "\n",
    "    FS=16000\n",
    "    NFFT=512\n",
    "\n",
    "    spec_sig=np.squeeze(spec_sig,axis=1)\n",
    "    nf=spec_sig.shape[0]\n",
    "    n_mfcc=np.shape(spec_sig)[1]\n",
    "    n_mel=np.shape(spec_sig)[2]\n",
    "\n",
    "    dctm=dct(n_mfcc,n_mel)\n",
    "    mel_basis=librosa.filters.mel(FS, NFFT)\n",
    "\n",
    "    # Empirical scaling of channels to get ~flat amplitude mapping.\n",
    "    bin_scaling=1.0/np.maximum(0.0005, np.sum(np.dot(mel_basis.T, mel_basis),axis=0))\n",
    "\n",
    "    for k in range(nf):\n",
    "    #         spec_sig_curr=np.squeeze(spec_sig[k,:,:],axis=0)\n",
    "        spec_sig_curr=spec_sig[k,:,:]\n",
    "\n",
    "        # Reconstruct the approximate STFT squared-magnitude from the MFCCs.\n",
    "        if k==0:\n",
    "            wav_stft=bin_scaling[:, np.newaxis] * np.dot(mel_basis.T,invlogamplitude(np.dot(dctm, spec_sig_curr.T)))\n",
    "#             wav_stft=librosa.feature.inverse.mel_to_stft(spec_sig_curr,sr=16000, n_fft=512)\n",
    "        else:\n",
    "            wav_stft=np.append(wav_stft,librosa.feature.inverse.mel_to_stft(spec_sig_curr,sr=16000, n_fft=512),axis=1)\n",
    "#             wav_stft=np.append(wav_stft,bin_scaling[:, np.newaxis] * np.dot(mel_basis.T,invlogamplitude(np.dot(dctm, spec_sig_curr.T))),axis=1)\n",
    "\n",
    "    # Impose reconstructed magnitude on white noise STFT.\n",
    "    excitation=np.random.randn(FS)\n",
    "    E=librosa.stft(excitation, n_fft=512)\n",
    "    #     if k==0:\n",
    "    speech_sig=librosa.core.griffinlim(np.dot((E/np.abs(E)).T,np.sqrt(wav_stft)).T)\n",
    "\n",
    "    sig_size=speech_sig.shape[0]\n",
    "    fr_samples=sig_size//(2*nf)\n",
    "    leftover=sig_size%fr_samples\n",
    "\n",
    "    output_speech_sig=np.zeros((fr_samples*(nf+1))+leftover)\n",
    "\n",
    "    c_idx=0\n",
    "\n",
    "    for k in range(nf):\n",
    "        if k==nf-1:\n",
    "            output_speech_sig[c_idx:]=speech_sig[c_idx*2:]\n",
    "        else:\n",
    "            output_speech_sig[c_idx:c_idx+fr_samples]=speech_sig[c_idx*2:c_idx*2+fr_samples]\n",
    "            c_idx+=fr_samples\n",
    "\n",
    "\n",
    "    return output_speech_sig\n",
    "\n",
    "\n",
    "#     else:\n",
    "# #         old_sig=speech_sig[c_idx:TIME_SHIFT]\n",
    "#         speech_sig=np.append(speech_sig,librosa.istft(np.dot((E/np.abs(E)).T,np.sqrt(wav_stft)).T))\n",
    "\n",
    "# return speech_sig \n",
    "\n",
    "# Output\n",
    "# librosa.output.write_wav('output.wav', ori_speech, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# librosa.output.write_wav('output.wav', speech_sig, FS)\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(speech_recon)\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(speech_ori)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(X, window_size, window_step):\n",
    "    \"\"\"\n",
    "    Create an overlapped version of X\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape=(n_samples,)\n",
    "        Input signal to window and overlap\n",
    "    window_size : int\n",
    "        Size of windows to take\n",
    "    window_step : int\n",
    "        Step size between windows\n",
    "    Returns\n",
    "    -------\n",
    "    X_strided : shape=(n_windows, window_size)\n",
    "        2D array of overlapped X\n",
    "    \"\"\"\n",
    "    if window_size % 2 != 0:\n",
    "        raise ValueError(\"Window size must be even!\")\n",
    "    # Make sure there are an even number of windows before stridetricks\n",
    "    append = np.zeros((window_size - len(X) % window_size))\n",
    "    X = np.hstack((X, append))\n",
    "\n",
    "    ws = window_size\n",
    "    ss = window_step\n",
    "    a = X\n",
    "\n",
    "    valid = len(a) - ws\n",
    "    nw = (valid) // ss\n",
    "    out = np.ndarray((nw, ws), dtype=a.dtype)\n",
    "\n",
    "    for i in np.arange(nw):\n",
    "        # \"slide\" the window along the samples\n",
    "        start = i * ss\n",
    "        stop = start + ws\n",
    "        out[i] = a[start:stop]\n",
    "\n",
    "    return out\n",
    "\n",
    "def stft(\n",
    "    X, fftsize=128, step=65, mean_normalize=True, real=False, compute_onesided=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute STFT for 1D real valued input X\n",
    "    \"\"\"\n",
    "    if real:\n",
    "        local_fft = np.fft.rfft\n",
    "        cut = -1\n",
    "    else:\n",
    "        local_fft = np.fft.fft\n",
    "        cut = None\n",
    "    if compute_onesided:\n",
    "        cut = fftsize // 2\n",
    "    if mean_normalize:\n",
    "        X -= X.mean()\n",
    "\n",
    "    X = overlap(X, fftsize, step)\n",
    "\n",
    "    size = fftsize\n",
    "    win = 0.54 - 0.46 * np.cos(2 * np.pi * np.arange(size) / (size - 1))\n",
    "    X = X * win[None]\n",
    "    X = local_fft(X)[:, :cut]\n",
    "    return X\n",
    "\n",
    "# Also mostly modified or taken from https://gist.github.com/kastnerkyle/179d6e9a88202ab0a2fe\n",
    "def invert_pretty_spectrogram(X_s, log=True, fft_size=512, step_size=512 / 4, n_iter=10):\n",
    "    \n",
    "    old_settings = np.seterr(over='ignore')\n",
    "    if log == True:\n",
    "        X_s = np.power(10, X_s)\n",
    "\n",
    "    X_s = np.concatenate([X_s, X_s[:, ::-1]], axis=1)\n",
    "    X_t = iterate_invert_spectrogram(X_s, fft_size, step_size, n_iter=n_iter)\n",
    "    return X_t\n",
    "\n",
    "\n",
    "def iterate_invert_spectrogram(X_s, fftsize, step, n_iter=10, verbose=False):\n",
    "    \"\"\"\n",
    "    Under MSR-LA License\n",
    "    Based on MATLAB implementation from Spectrogram Inversion Toolbox\n",
    "    References\n",
    "    ----------\n",
    "    D. Griffin and J. Lim. Signal estimation from modified\n",
    "    short-time Fourier transform. IEEE Trans. Acoust. Speech\n",
    "    Signal Process., 32(2):236-243, 1984.\n",
    "    Malcolm Slaney, Daniel Naar and Richard F. Lyon. Auditory\n",
    "    Model Inversion for Sound Separation. Proc. IEEE-ICASSP,\n",
    "    Adelaide, 1994, II.77-80.\n",
    "    Xinglei Zhu, G. Beauregard, L. Wyse. Real-Time Signal\n",
    "    Estimation from Modified Short-Time Fourier Transform\n",
    "    Magnitude Spectra. IEEE Transactions on Audio Speech and\n",
    "    Language Processing, 08/2007.\n",
    "    \"\"\"\n",
    "    reg = np.max(X_s) / 1e5\n",
    "    X_best = copy.deepcopy(X_s)\n",
    "    for i in range(n_iter):\n",
    "        if verbose:\n",
    "            print(\"Runnning iter %i\" % i)\n",
    "        if i == 0:\n",
    "            X_t = invert_spectrogram(\n",
    "                X_best, step, calculate_offset=True, set_zero_phase=True\n",
    "            )\n",
    "        else:\n",
    "            # Calculate offset was False in the MATLAB version\n",
    "            # but in mine it massively improves the result\n",
    "            # Possible bug in my impl?\n",
    "            X_t = invert_spectrogram(\n",
    "                X_best, step, calculate_offset=True, set_zero_phase=False\n",
    "            )\n",
    "        est = stft(X_t, fftsize=fftsize, step=step, compute_onesided=False)\n",
    "        phase = est / np.maximum(reg, np.abs(est))\n",
    "        X_best = X_s * phase[: len(X_s)]\n",
    "    X_t = invert_spectrogram(X_best, step, calculate_offset=True, set_zero_phase=False)\n",
    "    return np.real(X_t)\n",
    "\n",
    "\n",
    "def invert_spectrogram(X_s, step, calculate_offset=True, set_zero_phase=True):\n",
    "    \"\"\"\n",
    "    Under MSR-LA License\n",
    "    Based on MATLAB implementation from Spectrogram Inversion Toolbox\n",
    "    References\n",
    "    ----------\n",
    "    D. Griffin and J. Lim. Signal estimation from modified\n",
    "    short-time Fourier transform. IEEE Trans. Acoust. Speech\n",
    "    Signal Process., 32(2):236-243, 1984.\n",
    "    Malcolm Slaney, Daniel Naar and Richard F. Lyon. Auditory\n",
    "    Model Inversion for Sound Separation. Proc. IEEE-ICASSP,\n",
    "    Adelaide, 1994, II.77-80.\n",
    "    Xinglei Zhu, G. Beauregard, L. Wyse. Real-Time Signal\n",
    "    Estimation from Modified Short-Time Fourier Transform\n",
    "    Magnitude Spectra. IEEE Transactions on Audio Speech and\n",
    "    Language Processing, 08/2007.\n",
    "    \"\"\"\n",
    "    size = int(X_s.shape[1] // 2)\n",
    "    wave = np.zeros((X_s.shape[0] * step + size))\n",
    "    # Getting overflow warnings with 32 bit...\n",
    "    wave = wave.astype(\"float64\")\n",
    "    total_windowing_sum = np.zeros((X_s.shape[0] * step + size))\n",
    "    win = 0.54 - 0.46 * np.cos(2 * np.pi * np.arange(size) / (size - 1))\n",
    "\n",
    "    est_start = int(size // 2) - 1\n",
    "    est_end = est_start + size\n",
    "    for i in range(X_s.shape[0]):\n",
    "        wave_start = int(step * i)\n",
    "        wave_end = wave_start + size\n",
    "        if set_zero_phase:\n",
    "            spectral_slice = X_s[i].real + 0j\n",
    "        else:\n",
    "            # already complex\n",
    "            spectral_slice = X_s[i]\n",
    "\n",
    "        # Don't need fftshift due to different impl.\n",
    "        wave_est = np.real(np.fft.ifft(spectral_slice))[::-1]\n",
    "        if calculate_offset and i > 0:\n",
    "            offset_size = size - step\n",
    "            if offset_size <= 0:\n",
    "                print(\n",
    "                    \"WARNING: Large step size >50\\% detected! \"\n",
    "                    \"This code works best with high overlap - try \"\n",
    "                    \"with 75% or greater\"\n",
    "                )\n",
    "                offset_size = step\n",
    "            offset = xcorr_offset(\n",
    "                wave[wave_start : wave_start + offset_size],\n",
    "                wave_est[est_start : est_start + offset_size],\n",
    "            )\n",
    "        else:\n",
    "            offset = 0\n",
    "        wave[wave_start:wave_end] += (\n",
    "            win * wave_est[est_start - offset : est_end - offset]\n",
    "        )\n",
    "        total_windowing_sum[wave_start:wave_end] += win\n",
    "    wave = np.real(wave) / (total_windowing_sum + 1e-6)\n",
    "    return wave\n",
    "\n",
    "\n",
    "def xcorr_offset(x1, x2):\n",
    "    \"\"\"\n",
    "    Under MSR-LA License\n",
    "    Based on MATLAB implementation from Spectrogram Inversion Toolbox\n",
    "    References\n",
    "    ----------\n",
    "    D. Griffin and J. Lim. Signal estimation from modified\n",
    "    short-time Fourier transform. IEEE Trans. Acoust. Speech\n",
    "    Signal Process., 32(2):236-243, 1984.\n",
    "    Malcolm Slaney, Daniel Naar and Richard F. Lyon. Auditory\n",
    "    Model Inversion for Sound Separation. Proc. IEEE-ICASSP,\n",
    "    Adelaide, 1994, II.77-80.\n",
    "    Xinglei Zhu, G. Beauregard, L. Wyse. Real-Time Signal\n",
    "    Estimation from Modified Short-Time Fourier Transform\n",
    "    Magnitude Spectra. IEEE Transactions on Audio Speech and\n",
    "    Language Processing, 08/2007.\n",
    "    \"\"\"\n",
    "    x1 = x1 - x1.mean()\n",
    "    x2 = x2 - x2.mean()\n",
    "    frame_size = len(x2)\n",
    "    half = frame_size // 2\n",
    "    corrs = np.convolve(x1.astype(\"float32\"), x2[::-1].astype(\"float32\"))\n",
    "    corrs[:half] = -1e30\n",
    "    corrs[-half:] = -1e30\n",
    "    offset = corrs.argmax() - len(x1)\n",
    "    return offset\n",
    "\n",
    "\n",
    "def make_mel(spectrogram, mel_filter, shorten_factor=1):\n",
    "    mel_spec = np.transpose(mel_filter).dot(np.transpose(spectrogram))\n",
    "    mel_spec = scipy.ndimage.zoom(\n",
    "        mel_spec.astype(\"float32\"), [1, 1.0 / shorten_factor]\n",
    "    ).astype(\"float16\")\n",
    "    mel_spec = mel_spec[:, 1:-1]  # a little hacky but seemingly needed for clipping\n",
    "    return mel_spec\n",
    "\n",
    "\n",
    "def mel_to_spectrogram(mel_spec, mel_inversion_filter, spec_thresh, shorten_factor):\n",
    "    \"\"\"\n",
    "    takes in an mel spectrogram and returns a normal spectrogram for inversion \n",
    "    \"\"\"\n",
    "    mel_spec = mel_spec + spec_thresh\n",
    "    uncompressed_spec = np.transpose(np.transpose(mel_spec).dot(mel_inversion_filter))\n",
    "    uncompressed_spec = scipy.ndimage.zoom(\n",
    "        uncompressed_spec.astype(\"float32\"), [1, shorten_factor]\n",
    "    ).astype(\"float16\")\n",
    "    uncompressed_spec = uncompressed_spec - 4\n",
    "    return uncompressed_spec\n",
    "\n",
    "\n",
    "# From https://github.com/jameslyons/python_speech_features\n",
    "\n",
    "\n",
    "def hz2mel(hz):\n",
    "    \"\"\"Convert a value in Hertz to Mels\n",
    "    :param hz: a value in Hz. This can also be a numpy array, conversion proceeds element-wise.\n",
    "    :returns: a value in Mels. If an array was passed in, an identical sized array is returned.\n",
    "    \"\"\"\n",
    "    return 2595 * np.log10(1 + hz / 700.0)\n",
    "\n",
    "\n",
    "def mel2hz(mel):\n",
    "    \"\"\"Convert a value in Mels to Hertz\n",
    "    :param mel: a value in Mels. This can also be a numpy array, conversion proceeds element-wise.\n",
    "    :returns: a value in Hertz. If an array was passed in, an identical sized array is returned.\n",
    "    \"\"\"\n",
    "    return 700 * (10 ** (mel / 2595.0) - 1)\n",
    "\n",
    "\n",
    "def get_filterbanks(nfilt=20, nfft=512, samplerate=16000, lowfreq=0, highfreq=None):\n",
    "    \"\"\"Compute a Mel-filterbank. The filters are stored in the rows, the columns correspond\n",
    "    to fft bins. The filters are returned as an array of size nfilt * (nfft/2 + 1)\n",
    "    :param nfilt: the number of filters in the filterbank, default 20.\n",
    "    :param nfft: the FFT size. Default is 512.\n",
    "    :param samplerate: the samplerate of the signal we are working with. Affects mel spacing.\n",
    "    :param lowfreq: lowest band edge of mel filters, default 0 Hz\n",
    "    :param highfreq: highest band edge of mel filters, default samplerate/2\n",
    "    :returns: A numpy array of size nfilt * (nfft/2 + 1) containing filterbank. Each row holds 1 filter.\n",
    "    \"\"\"\n",
    "    highfreq = highfreq or samplerate / 2\n",
    "    assert highfreq <= samplerate / 2, \"highfreq is greater than samplerate/2\"\n",
    "\n",
    "    # compute points evenly spaced in mels\n",
    "    lowmel = hz2mel(lowfreq)\n",
    "    highmel = hz2mel(highfreq)\n",
    "    melpoints = np.linspace(lowmel, highmel, nfilt + 2)\n",
    "    # our points are in Hz, but we use fft bins, so we have to convert\n",
    "    #  from Hz to fft bin number\n",
    "    bin = np.floor((nfft + 1) * mel2hz(melpoints) / samplerate)\n",
    "\n",
    "    fbank = np.zeros([nfilt, nfft // 2])\n",
    "    for j in range(0, nfilt):\n",
    "        for i in range(int(bin[j]), int(bin[j + 1])):\n",
    "            fbank[j, i] = (i - bin[j]) / (bin[j + 1] - bin[j])\n",
    "        for i in range(int(bin[j + 1]), int(bin[j + 2])):\n",
    "            fbank[j, i] = (bin[j + 2] - i) / (bin[j + 2] - bin[j + 1])\n",
    "    return fbank\n",
    "\n",
    "\n",
    "def create_mel_filter(\n",
    "    fft_size, n_freq_components=64, start_freq=300, end_freq=8000, samplerate=16000\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a filter to convolve with the spectrogram to get out mels\n",
    "\n",
    "    \"\"\"\n",
    "    mel_inversion_filter = get_filterbanks(\n",
    "        nfilt=n_freq_components,\n",
    "        nfft=fft_size,\n",
    "        samplerate=samplerate,\n",
    "        lowfreq=start_freq,\n",
    "        highfreq=end_freq,\n",
    "    )\n",
    "    # Normalize filter\n",
    "    ax1_sum=np.where(mel_inversion_filter.sum(axis=1)==0, mel_inversion_filter.sum(axis=1)==0, 0.005)\n",
    "    mel_filter = mel_inversion_filter.T / ax1_sum\n",
    "\n",
    "    return mel_filter, mel_inversion_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "spec_thresh=4\n",
    "shorten_factor=10\n",
    "\n",
    "mel_filter, mel_inversion_filter = create_mel_filter(\n",
    "    fft_size=NFFT,\n",
    "    n_freq_components=128,\n",
    "    start_freq=0,\n",
    "    end_freq=8000,\n",
    ")\n",
    "\n",
    "mel_inverted_spectrogram = mel_to_spectrogram(\n",
    "    spec_ori[4,0,:,:],\n",
    "    mel_inversion_filter,\n",
    "    spec_thresh=spec_thresh,\n",
    "    shorten_factor=shorten_factor,\n",
    ")\n",
    "\n",
    "inverted_mel_audio = invert_pretty_spectrogram(\n",
    "    np.transpose(mel_inverted_spectrogram),\n",
    "    fft_size=512,\n",
    "    step_size=32,\n",
    "    log=True,\n",
    "    n_iter=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mel2speech(spec_signal):\n",
    "# spec_signal= spec_ori\n",
    "# feats = librosa.feature.mfcc(S=librosa.power_to_db(spec_ori), n_mfcc=spec_ori.shape[0])\n",
    "# FRAME_SIZE=0.5\n",
    "# TIME_SHIFT=0.25\n",
    "# OVRLAP=FRAME_SIZE/TIME_SHIFT\n",
    "# FS=16000\n",
    "# NFFT=512  \n",
    "# HOP=64\n",
    "\n",
    "# signal=signal-np.mean(signal)\n",
    "# signal=signal/np.max(np.abs(signal))\n",
    "# init=0\n",
    "# endi=int(FRAME_SIZE*self.fs)\n",
    "# nf=spec_signal.shape[0]\n",
    "# signal_len=(nf+1)*(TIME_SHIFT*FS)\n",
    "# spec_signal=np.squeeze(spec_signal,axis=1)\n",
    "\n",
    "# j=0\n",
    "# for k in range(nf):\n",
    "#     try:\n",
    "# #         sig_curr=np.squeeze(spec_signal[k,:,:],axis=0)\n",
    "#         sig_curr=spec_signal[k,:,:]\n",
    "#         inv_imag=librosa.feature.inverse.mel_to_stft(sig_curr,sr=FS,n_fft=NFFT, hop_length=HOP)\n",
    "        \n",
    "#         frame=signal[init:endi]\n",
    "#         imag=melspectrogram(frame, sr=self.fs, n_fft=NFFT, hop_length=HOP, n_mels=self.nmels, fmax=self.fs/2)\n",
    "#         init=init+int(TIME_SHIFT*self.fs)\n",
    "#         endi=endi+int(TIME_SHIFT*self.fs)\n",
    "#         if np.min(np.min(imag))<=0:\n",
    "#             warnings.warn(\"There is Inf values in the Mel spectrogram\")\n",
    "#             continue\n",
    "#         imag=np.log(imag, dtype=np.float32)\n",
    "#         imagt=torch.from_numpy(imag)\n",
    "#         mat[j,:,:,:]=imagt\n",
    "#         j+=1\n",
    "        \n",
    "# spec_signal=spec_signal.reshape(spec_signal.shape[1],-1)\n",
    "# spec_signal=np.squeeze(spec_signal,axis=1)\n",
    "# speech_signal=librosa.feature.inverse.mel_to_stft(spec_signal,128)\n",
    "# speech_signal\n",
    "# speech_signal=np.array([])\n",
    "# num_batches=np.shape(spec_signal)[0]\n",
    "\n",
    "# return speech_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "\n",
    "PATH=os.getcwd()\n",
    "path_audio=PATH+'/tedx_spanish_corpus/speech/test/'\n",
    "wav_file=path_audio+os.listdir(path_audio)[0]\n",
    "fs, signal=read(wav_file)\n",
    "FS=16000\n",
    "\n",
    "obj = wave.open('sound.wav','w')\n",
    "obj.setnchannels(1) # mono\n",
    "obj.setsampwidth(2)\n",
    "obj.setframerate(FS)\n",
    "# for i in range(99999):\n",
    "#    value = random.randint(-32767, 32767)\n",
    "#    data = struct.pack('<h', value)\n",
    "obj.writeframesraw(speech_ori)\n",
    "obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aespeech=AEspeech(\"CAE\", 256, rep='wvlt') # load the pretrained CAE with 256 units\n",
    "# mat_spec=aespeech.compute_spectrograms(wav_file) # compute the decoded spectrograms from the autoencoder\n",
    "# print(mat_spec.size())\n",
    "# aespeech.show_spectrograms(mat_spec)\n",
    "\n",
    "coefs,freqs=aespeech.compute_cwt(wav_file,volta=2)\n",
    "time=np.arange(np.shape(coefs)[1])\n",
    "ax=aespeech.show_scalogram(time,coefs,freqs, yaxis='frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logmmseDenoise(x):\n",
    "\n",
    "#     x=speech_ori\n",
    "\n",
    "    #Define frame size, time shift in terms of samples\n",
    "    FRAME_SIZE=500  \n",
    "    TIME_SHIFT=250\n",
    "    NFFT=512\n",
    "    FS=16000\n",
    "    win=np.hamming(FRAME_SIZE) \n",
    "\n",
    "    x_old=np.zeros((TIME_SHIFT,NFFT))  \n",
    "    Nframes=int(np.floor(len(x)/TIME_SHIFT)-(FRAME_SIZE/TIME_SHIFT)) \n",
    "    xfinal=np.zeros((Nframes*TIME_SHIFT,NFFT))\n",
    "\n",
    "    noise_mean=np.zeros((NFFT,1))  \n",
    "    j=0  \n",
    "    for m in range(2):\n",
    "        noise_mean=noise_mean+abs(scipy.fft(np.multiply(win,x[j:j+FRAME_SIZE]),NFFT))\n",
    "        j=j+FRAME_SIZE  \n",
    "\n",
    "    noise_mu=noise_mean/6  \n",
    "    noise_mu2=noise_mu**2  \n",
    "\n",
    "    k=0  \n",
    "    aa=0.98  \n",
    "    mu=0.98  \n",
    "    eta=0.15   \n",
    "\n",
    "    ksi_min=10**(-25/10)  \n",
    "\n",
    "    nf=int(len(x)/(TIME_SHIFT*FS))-1\n",
    "    \n",
    "    for n in range(nf):\n",
    "\n",
    "        insign=np.multiply(win,x[k:k+FRAME_SIZE])  \n",
    "\n",
    "        #compute magnitude square of fft\n",
    "        spec=scipy.fft(insign,NFFT) \n",
    "        sig=abs(spec)  \n",
    "        sig2=sig**2  \n",
    "\n",
    "        # limit post SNR to avoid overflows\n",
    "        gammak=np.divide(sig2,noise_mu2)\n",
    "        gammak=np.where(gammak>40,40,gammak)\n",
    "\n",
    "\n",
    "        temp_gammak=gammak-1\n",
    "        temp_gammak=np.where(temp_gammak<0,0,temp_gammak)\n",
    "        if n==0:  \n",
    "            ksi=aa+np.multiply((1-aa),temp_gammak)\n",
    "        else:  \n",
    "            #a-priori SNR with ksi limited to -25 dB\n",
    "            ksi=np.divide(aa*Xk_prev,noise_mu2) + np.multiply((1-aa),temp_gammak)\n",
    "            ksi=np.where(ksi<ksi_min,ksi_min,ksi)\n",
    "    #         ksi=[max(ksi_min,k) for ks in ksi for k in ks]     \n",
    "\n",
    "        log_sigma_k=np.divide(gammak,np.divide(ksi,(1+ ksi)- np.log(1+ ksi)))      \n",
    "        vad_decision= np.divide(sum(log_sigma_k),FRAME_SIZE)      \n",
    "\n",
    "        #If only noise in frame found\n",
    "        if np.all(vad_decision<eta):   \n",
    "            noise_mu2=mu*noise_mu2+(1-mu)*sig2\n",
    "\n",
    "        #Log-MMSE estimator  \n",
    "        A=np.divide(ksi,(1+ksi)) \n",
    "        vk=np.multiply(A,gammak)  \n",
    "        ei_vk=0.5*scipy.special.exp1(vk)  \n",
    "        hw=np.multiply(A,np.exp(ei_vk))\n",
    "\n",
    "        sig=np.multiply(sig,hw)  \n",
    "        Xk_prev=sig**2  \n",
    "\n",
    "        xi_w=scipy.ifft(np.multiply(hw,spec),NFFT)  \n",
    "        xi_w=np.real(xi_w)\n",
    "\n",
    "        xfinal[:TIME_SHIFT]=x_old+xi_w[:TIME_SHIFT]\n",
    "        x_old=xi_w[TIME_SHIFT:FRAME_SIZE]\n",
    "\n",
    "        k=k+TIME_SHIFT\n",
    "\n",
    "    return xfinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat=aespeech.compute_cwt(wav_file,volta=1)\n",
    "# mat=aespeech.standard(mat)\n",
    "if torch.cuda.is_available():\n",
    "    mat=mat.cuda()\n",
    "to,bot=aespeech.AE.forward(mat)\n",
    "# to=aespeech.destandard(to)\n",
    "\n",
    "recon=to.data.numpy()\n",
    "ori=mat.data.numpy()\n",
    "\n",
    "recon=np.reshape(recon,(np.shape(recon)[2:]))\n",
    "ori=np.reshape(ori,(np.shape(ori)[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time=np.arange(np.shape(recon)[-1])\n",
    "ax=aespeech.show_scalogram(time,ori,freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time=np.arange(np.shape(recon)[-1])\n",
    "ax=aespeech.show_scalogram(time,recon,freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_PERIOD = 3200\n",
    "HOP = 20\n",
    "fs,data=read(wav_file)\n",
    "\n",
    "time=np.arange(data.shape[0s])\n",
    "scales = np.arange(1, SAMPLE_PERIOD, HOP)*pywt.central_frequency('morl')\n",
    "                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwt= scg.CWT(time,data,scales)\n",
    "cmp_coefs=cwt.coefs\n",
    "freqs=cwt.scales_freq\n",
    "# coefs, freqs = pywt.cwt(data, scales, 'morl')\n",
    "np.abs(cmp_coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat_spec=aespeech.compute_spectrograms(wav_file) # compute the decoded spectrograms from the autoencoder\n",
    "# print(mat_spec.size())\n",
    "# aespeech.show_spectrograms(mat_spec)\n",
    "aespeech.min_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_PERIOD = 3200\n",
    "HOP = 25\n",
    "# coefs = wv_coefs.data.numpy()[0,0,:,:]\n",
    "\n",
    "# signal=pywt.icwt(coefs, None, 'morl', 'smooth')\n",
    "uns_signal_length = np.shape(signal)[0]\n",
    "\n",
    "\n",
    "signal_new = signal_new - np.mean(signal_new)\n",
    "signal_new = signal_new/np.max(np.abs(signal_new))\n",
    "signal_new_length = np.shape(signal_new)[0]\n",
    "xtix = np.array((np.arange(uns_signal_length)/16000))\n",
    "\n",
    "# range of scales to perform the transform\n",
    "scales = np.arange(1, SAMPLE_PERIOD, HOP)*pywt.central_frequency('morl')\n",
    "\n",
    "ax2 = scg.cws(signal_new[:signal_new_length], scales=scales, figsize=(10, 4.0), yscale = 'log', coi = False, ylabel=\"Period\", xlabel=\"Time (s)\")\n",
    "#         ax2.set_title=wav_file.split('/')[-1]\n",
    "ax2.set_xticks(np.arange(0,16000,signal_new_length//np.ceil(xtix[-1])))\n",
    "ax2.set_xticklabels(np.round(xtix[::int(uns_signal_length//int(np.ceil(xtix[-1])))],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose default wavelet function \n",
    "waveletype = 'morl'\n",
    "sample_period = 3200\n",
    "hop = 64\n",
    "fs_new = 16000\n",
    "\n",
    "signal_new = signal_new - np.mean(signal_new)\n",
    "signal_new = signal_new/np.max(np.abs(signal_new))\n",
    "\n",
    "# range of scales to perform the transform\n",
    "scales =  np.arange(1, sample_period, hop)*pywt.central_frequency(waveletype)\n",
    "coefs, freqs = pywt.cwt(signal_new, scales, waveletype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose default wavelet function \n",
    "waveletype = 'morl'\n",
    "\n",
    "fs_new = 16000\n",
    "signal_new = signal - np.mean(signal)\n",
    "signal_new = signal_new/np.max(np.abs(signal_new))\n",
    "signal_new_length = np.shape(signal_new)[0]\n",
    "uns_signal_length = np.shape(signal)[0]\n",
    "xtix = np.array((np.arange(uns_signal_length)/fs))\n",
    "\n",
    "\n",
    "# range of scales to perform the transform\n",
    "scales = np.logspace(np.log10(2), np.log10(3200), 64)*pywt.central_frequency(waveletype)\n",
    "\n",
    "\n",
    "x_values_wvt_arr = range(0,signal_new_length,1)\n",
    "\n",
    "# plot the signal \n",
    "fig1, ax1 = plt.subplots(1, 1, figsize=(9, 3.5));  \n",
    "ax1.plot(x_values_wvt_arr, signal_new[:signal_new_length], linewidth=3, color='blue')\n",
    "ax1.set_xlim(0, signal_new_length)\n",
    "ax1.set_xlabel(\"Time (s)\")\n",
    "ax1.set_ylabel(\"Normalized Freq. (Hz)\")\n",
    "ax1.set_title(wav_file.split('/')[-1])\n",
    "ax1.set_xticks(np.arange(0,fs_new,signal_new_length//np.ceil(xtix[-1])))\n",
    "ax1.set_xticklabels(np.round(xtix[0:-1:int(uns_signal_length//int(np.ceil(xtix[-1])))],2))\n",
    "\n",
    "# the scaleogram\n",
    "ax2 = scg.cws(signal_new[:signal_new_length], scales=scales, figsize=(10, 4.0), yscale = 'log',coi = False, ylabel=\"Period\", xlabel=\"Time (s)\",\n",
    "        title=wav_file.split('/')[-1])\n",
    "ax2.set_xticks(np.arange(0,fs_new,signal_new_length//np.ceil(xtix[-1])))\n",
    "ax2.set_xticklabels(np.round(xtix[::int(uns_signal_length//int(np.ceil(xtix[-1])))],2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
